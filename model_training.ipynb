{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d81619054298cb84",
   "metadata": {},
   "source": [
    "# T2A-LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9678cda37dd5de34",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from transformers import ClapModel, ClapProcessor\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "from config import DatasetConfig\n",
    "from libritts import LIBRITTSP\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "from librosa import display as rosa_display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df4c9ee916b8505",
   "metadata": {},
   "source": [
    "### Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a9a659920805ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7cad9ff7ebd6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDA Device Number\n",
    "DEVICE_NUM = 0\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(f\"cuda:{DEVICE_NUM}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    DEVICE_NUM = -1\n",
    "\n",
    "print(f\"INFO: Using device - {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ca176a8586463d",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb64ef2f4d1f2688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_waveform(audio_path: str | None, waveform: torch.Tensor | None = None, sr: int = 48000):\n",
    "    if audio_path:\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "    elif waveform is not None:\n",
    "        waveform = waveform.unsqueeze(0) if len(waveform.shape) == 1 else waveform\n",
    "    else:\n",
    "        raise ValueError(\"Either audio_path or waveform must be provided.\")\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    rosa_display.waveshow(waveform.numpy()[0], sr=sr)\n",
    "    plt.title(\"Waveform\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    return Audio(waveform.numpy()[0], rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701d2b0c7fca1ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = LIBRITTSP(config=DatasetConfig(name=\"libritts\", sample_rate=24000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a7e6ea728b09c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d837b4c4331af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample['content_prompt'])\n",
    "print(sample['style_prompt'])\n",
    "show_waveform(None, sample['audio'], sr=sample['sampling_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef36a00b89ab989e",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b22b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parler_tts import ParlerTTSForConditionalGeneration\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d730baf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_id = \"parler-tts/parler-tts-mini-v1\"\n",
    "clap_model_id = \"laion/larger_clap_general\"  # \"microsoft/msclap\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5e3558c9cf041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check base model architecture\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "base_model = ParlerTTSForConditionalGeneration.from_pretrained(base_model_id)\n",
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948e2cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6174b2cb6f3dfd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clap_processor = ClapProcessor.from_pretrained(clap_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b4778a22d86c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check clap model architecture\n",
    "clap_model = ClapModel.from_pretrained(clap_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b041841",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in clap_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c0b7b8507108c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clap_model.text_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424d7bc17e049325",
   "metadata": {},
   "source": [
    "## LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324c0e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Model Structure ===\")\n",
    "for name, module in base_model.named_modules():\n",
    "    if isinstance(module, nn.Linear) and 'decoder' in name:\n",
    "        print(f\"{name}: {module}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb91b391c60bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\n",
    "        \"self_attn.q_proj\",\n",
    "        \"self_attn.k_proj\",\n",
    "        \"self_attn.v_proj\",\n",
    "        \"self_attn.out_proj\",\n",
    "        \"fc1\",\n",
    "        \"fc2\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"LoRA config created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d70044311a05ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(base_model, lora_config)\n",
    "peft_model.print_trainable_parameters()\n",
    "\n",
    "print(\"\\nINFO: LoRA applied successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1713e9b4c874efd",
   "metadata": {},
   "source": [
    "## Hypernetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8a037b9d0b041",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperNetwork(nn.Module):\n",
    "    \"\"\"HyperNetwork that generates LoRA weights from CLAP embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, clap_dim=512, hidden_dim=1024, lora_params_dict=None):\n",
    "        super().__init__()\n",
    "        self.clap_dim = clap_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lora_params_dict = lora_params_dict or {}\n",
    "        \n",
    "        # Shared embedding processor\n",
    "        self.embedding_processor = nn.Sequential(\n",
    "            nn.Linear(clap_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        \n",
    "        # Create generators for each LoRA parameter\n",
    "        self.weight_generators = nn.ModuleDict()\n",
    "        \n",
    "        for param_name, shape in self.lora_params_dict.items():\n",
    "            total_elements = shape.numel()\n",
    "            \n",
    "            self.weight_generators[param_name.replace('.', '_')] = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Linear(hidden_dim // 2, total_elements),\n",
    "            )\n",
    "    \n",
    "    def forward(self, clap_embedding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            clap_embedding: (batch_size, clap_dim)\n",
    "        Returns:\n",
    "            dict of LoRA weights\n",
    "        \"\"\"\n",
    "        # Process CLAP embedding\n",
    "        h = self.embedding_processor(clap_embedding)  # (batch_size, hidden_dim)\n",
    "        \n",
    "        # Generate LoRA weights\n",
    "        lora_weights = {}\n",
    "        for param_name, shape in self.lora_params_dict.items():\n",
    "            param_key = param_name.replace('.', '_')\n",
    "            flat_weights = self.weight_generators[param_key](h)  # (batch_size, total_elements)\n",
    "            \n",
    "            # Reshape to original shape\n",
    "            batch_size = flat_weights.shape[0]\n",
    "            lora_weights[param_name] = flat_weights.view(batch_size, *shape)\n",
    "        \n",
    "        return lora_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dced6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect LoRA parameter shapes\n",
    "lora_params_dict = {}\n",
    "for name, param in peft_model.named_parameters():\n",
    "    if 'lora' in name and param.requires_grad:\n",
    "        lora_params_dict[name] = param.shape\n",
    "        print(f\"{name}: {param.shape}\")\n",
    "\n",
    "print(f\"\\nTotal LoRA parameters to generate: {len(lora_params_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ba2138d940a032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HyperNetwork\n",
    "clap_embedding_dim = clap_model.config.projection_dim  # Usually 512\n",
    "hypernetwork = HyperNetwork(\n",
    "    clap_dim=clap_embedding_dim,\n",
    "    hidden_dim=1024,\n",
    "    lora_params_dict=lora_params_dict\n",
    ").to(device)\n",
    "\n",
    "print(f\"HyperNetwork parameters: {sum(p.numel() for p in hypernetwork.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982cf229",
   "metadata": {},
   "source": [
    "## Total Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3f6776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCCLoss(nn.Module):\n",
    "    \"\"\"Loss based on MFCC similarity between generated and target audio\"\"\"\n",
    "\n",
    "    def __init__(self, sample_rate=24000, n_mfcc=40, n_mels=80):\n",
    "        super().__init__()\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mfcc = n_mfcc\n",
    "        \n",
    "        # MFCC transform\n",
    "        self.mfcc_transform = T.MFCC(\n",
    "            sample_rate=sample_rate,\n",
    "            n_mfcc=n_mfcc,\n",
    "            melkwargs={\n",
    "                'n_fft': 1024,\n",
    "                'hop_length': 256,\n",
    "                'n_mels': n_mels,\n",
    "                'center': False\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def extract_mfcc(self, waveform):\n",
    "        \"\"\"\n",
    "        Extract MFCC features from waveform\n",
    "        Args:\n",
    "            waveform: (batch_size, n_samples) or (batch_size, 1, n_samples)\n",
    "        Returns:\n",
    "            mfcc: (batch_size, n_mfcc, time)\n",
    "        \"\"\"\n",
    "        if waveform.dim() == 3 and waveform.shape[1] == 1:\n",
    "            waveform = waveform.squeeze(1)\n",
    "        \n",
    "        mfcc = self.mfcc_transform(waveform)\n",
    "        return mfcc\n",
    "    \n",
    "    def forward(self, generated_audio, target_audio):\n",
    "        \"\"\"\n",
    "        Compute MFCC-based loss\n",
    "        Args:\n",
    "            generated_audio: (batch_size, n_samples)\n",
    "            target_audio: (batch_size, n_samples)\n",
    "        Returns:\n",
    "            loss: scalar\n",
    "        \"\"\"\n",
    "        # Extract MFCCs\n",
    "        gen_mfcc = self.extract_mfcc(generated_audio)  # (B, n_mfcc, T1)\n",
    "        tgt_mfcc = self.extract_mfcc(target_audio)     # (B, n_mfcc, T2)\n",
    "        \n",
    "        # Align temporal dimensions (take minimum length)\n",
    "        min_time = min(gen_mfcc.shape[-1], tgt_mfcc.shape[-1])\n",
    "        gen_mfcc = gen_mfcc[..., :min_time]\n",
    "        tgt_mfcc = tgt_mfcc[..., :min_time]\n",
    "        \n",
    "        # Compute L1 loss (better for audio than L2)\n",
    "        mfcc_loss = F.l1_loss(gen_mfcc, tgt_mfcc)\n",
    "        \n",
    "        # Optional: Add cosine similarity loss for phase alignment\n",
    "        gen_flat = gen_mfcc.flatten(1)  # (B, n_mfcc*T)\n",
    "        tgt_flat = tgt_mfcc.flatten(1)\n",
    "        cosine_sim = F.cosine_similarity(gen_flat, tgt_flat, dim=1).mean()\n",
    "        cosine_loss = 1 - cosine_sim\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = mfcc_loss + 0.1 * cosine_loss\n",
    "        \n",
    "        return total_loss, {\n",
    "            'mfcc_l1': mfcc_loss.item(),\n",
    "            'cosine_loss': cosine_loss.item(),\n",
    "            'total': total_loss.item()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053f12a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_loss_fn = MFCCLoss(sample_rate=24000).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95065a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T2ALoRAModel(nn.Module):\n",
    "    \"\"\"Complete Text-to-Audio LoRA model with MFCC-based training\"\"\"\n",
    "    \n",
    "    def __init__(self, clap_model, clap_processor, hypernetwork, peft_model, base_tokenizer):\n",
    "        super().__init__()\n",
    "        self.clap_model = clap_model\n",
    "        self.clap_processor = clap_processor\n",
    "        self.hypernetwork = hypernetwork\n",
    "        self.peft_model = peft_model\n",
    "        self.base_tokenizer = base_tokenizer\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        \n",
    "        # Freeze CLAP and base Parler-TTS\n",
    "        for param in self.clap_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for name, param in self.peft_model.named_parameters():\n",
    "            if 'lora' not in name:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        super().to(*args, **kwargs)\n",
    "        for arg in args:\n",
    "            if isinstance(arg, torch.device):\n",
    "                self.device = arg\n",
    "        for kwarg in kwargs.values():\n",
    "            if isinstance(kwarg, torch.device):\n",
    "                self.device = arg\n",
    "\n",
    "    def get_clap_embeddings(self, text_descriptions):\n",
    "        \"\"\"Get CLAP text embeddings for voice descriptions\"\"\"\n",
    "        # text_descriptions가 리스트면 첫 번째 요소만, 문자열이면 그대로 사용\n",
    "        if isinstance(text_descriptions, list):\n",
    "            text = text_descriptions[0] if text_descriptions else \"\"\n",
    "        else:\n",
    "            text = text_descriptions\n",
    "        \n",
    "        # 단일 문자열로 처리\n",
    "        inputs = self.clap_processor(\n",
    "            text=text,  # 리스트가 아닌 문자열로 전달\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            text_embeds = self.clap_model.get_text_features(**inputs)\n",
    "        \n",
    "        return text_embeds\n",
    "    \n",
    "    def apply_lora_weights(self, lora_weights, batch_idx=0):\n",
    "        \"\"\"Apply generated LoRA weights to the model\"\"\"\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.peft_model.named_parameters():\n",
    "                if 'lora' in name and param.requires_grad:\n",
    "                    if name in lora_weights:\n",
    "                        # Use weights for this batch sample\n",
    "                        generated_weight = lora_weights[name][batch_idx]\n",
    "                        param.copy_(generated_weight)\n",
    "    \n",
    "    def generate_audio(self, input_texts, voice_descriptions):\n",
    "        \"\"\"\n",
    "        Generate audio with dynamic LoRA weights\n",
    "        Args:\n",
    "            input_texts: list of strings\n",
    "            voice_descriptions: list of strings\n",
    "        Returns:\n",
    "            generated_audios: list of tensors\n",
    "        \"\"\"\n",
    "        # Get CLAP embeddings\n",
    "        clap_embeds = self.get_clap_embeddings(voice_descriptions)\n",
    "        \n",
    "        # Generate LoRA weights\n",
    "        lora_weights = self.hypernetwork(clap_embeds)\n",
    "        \n",
    "        # Tokenize input texts\n",
    "        inputs = self.base_tokenizer(\n",
    "            input_texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        ).to(self.peft_model.device)\n",
    "        \n",
    "        # Generate audio for each sample in batch\n",
    "        generated_audios = []\n",
    "        batch_size = len(input_texts)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # Apply LoRA weights for this sample\n",
    "            self.apply_lora_weights(lora_weights, batch_idx=i)\n",
    "            \n",
    "            # Generate audio\n",
    "            with torch.no_grad():\n",
    "                generation = self.peft_model.generate(\n",
    "                    input_ids=inputs.input_ids[i:i+1],\n",
    "                    attention_mask=inputs.attention_mask[i:i+1],\n",
    "                    # Empty prompt - we don't use Parler's text styling\n",
    "                    prompt_input_ids=None,\n",
    "                    prompt_attention_mask=None,\n",
    "                    do_sample=True,\n",
    "                    temperature=1.0,\n",
    "                    max_length=2000,\n",
    "                )\n",
    "            \n",
    "            # Decode to audio\n",
    "            audio_arr = self.peft_model.audio_encoder.decode(\n",
    "                generation,\n",
    "                audio_scales=[None]\n",
    "            )\n",
    "            audio = torch.from_numpy(audio_arr).squeeze(0)\n",
    "            generated_audios.append(audio)\n",
    "        \n",
    "        return generated_audios, lora_weights\n",
    "    \n",
    "    def forward(self, voice_descriptions, input_texts, target_audios=None):\n",
    "        \"\"\"\n",
    "        Forward pass with MFCC-based loss\n",
    "        Args:\n",
    "            voice_descriptions: list of strings describing voice style\n",
    "            input_texts: list of strings to synthesize\n",
    "            target_audios: (batch_size, n_samples) target audio waveforms\n",
    "        Returns:\n",
    "            dict with loss and generated audios\n",
    "        \"\"\"\n",
    "        # Generate audio\n",
    "        generated_audios, lora_weights = self.generate_audio(input_texts, voice_descriptions)\n",
    "        \n",
    "        # If training, compute loss\n",
    "        if target_audios is not None and self.training:\n",
    "            # Stack generated audios\n",
    "            # Need to pad/trim to same length\n",
    "            max_len = max(audio.shape[-1] for audio in generated_audios)\n",
    "            padded_gen = []\n",
    "            for audio in generated_audios:\n",
    "                if audio.shape[-1] < max_len:\n",
    "                    padding = max_len - audio.shape[-1]\n",
    "                    audio = F.pad(audio, (0, padding))\n",
    "                else:\n",
    "                    audio = audio[..., :max_len]\n",
    "                padded_gen.append(audio)\n",
    "            \n",
    "            gen_batch = torch.stack(padded_gen).to(target_audios.device)\n",
    "            \n",
    "            # Align target audio length\n",
    "            if target_audios.shape[-1] != max_len:\n",
    "                if target_audios.shape[-1] < max_len:\n",
    "                    padding = max_len - target_audios.shape[-1]\n",
    "                    target_audios = F.pad(target_audios, (0, padding))\n",
    "                else:\n",
    "                    target_audios = target_audios[..., :max_len]\n",
    "            \n",
    "            # Compute MFCC loss\n",
    "            loss, loss_dict = mfcc_loss_fn(gen_batch, target_audios)\n",
    "            \n",
    "            return {\n",
    "                'loss': loss,\n",
    "                'loss_dict': loss_dict,\n",
    "                'generated_audios': generated_audios,\n",
    "                'lora_weights': lora_weights\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'generated_audios': generated_audios,\n",
    "                'lora_weights': lora_weights\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f631a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_model = T2ALoRAModel(clap_model, clap_processor, hypernetwork, peft_model, base_tokenizer)\n",
    "voice_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a375568c6ddc2059",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cafc886d53d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "BATCH_SIZE = 2  # Reduced due to audio generation overhead\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "WARMUP_STEPS = 500\n",
    "SAVE_STEPS = 500\n",
    "LOG_STEPS = 50\n",
    "OUTPUT_DIR = \"./t2a_lora_mfcc_checkpoints\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba28fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    datasets,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=lambda x: x\n",
    ")\n",
    "\n",
    "print(f\"Total training batches: {len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb09013cf19fb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer - only train hypernetwork\n",
    "optimizer = AdamW(\n",
    "    hypernetwork.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "total_steps = len(train_dataloader) * NUM_EPOCHS\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=total_steps,\n",
    "    eta_min=1e-6\n",
    ")\n",
    "\n",
    "print(f\"Optimizer configured: {sum(p.numel() for p in hypernetwork.parameters()):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b0cc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "DUMMY = \"A female speaker delivers a slightly expressive and animated speech with a moderate speed and pitch. The recording is of very high quality, with the speaker's voice sounding clear and very close up.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9f9d54cc23549f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with MFCC-based loss\n",
    "global_step = 0\n",
    "best_loss = float('inf')\n",
    "\n",
    "print(\"Starting MFCC-based training...\\n\")\n",
    "print(\"Strategy: Compare generated audio vs target audio using MFCC features\")\n",
    "print(\"Goal: LoRA learns to modify voice style while preserving content\\n\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    voice_model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_mfcc_l1 = 0\n",
    "    epoch_cosine = 0\n",
    "    \n",
    "    pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        batch_loss = 0\n",
    "        batch_mfcc_l1 = 0\n",
    "        batch_cosine = 0\n",
    "        \n",
    "        # Process each sample individually (no batching for generation)\n",
    "        for item in batch:\n",
    "            voice_description = item['style_prompt']\n",
    "            input_text = item['content_prompt']\n",
    "            target_audio = item['audio'].to(device)\n",
    "            \n",
    "            # Ensure target is 1D\n",
    "            if target_audio.dim() > 1:\n",
    "                target_audio = target_audio.squeeze()\n",
    "            \n",
    "            # Get CLAP embeddings\n",
    "            clap_embeds = voice_model.get_clap_embeddings([voice_description])\n",
    "            \n",
    "            # Generate LoRA weights\n",
    "            lora_weights = hypernetwork(clap_embeds)\n",
    "            \n",
    "            # Apply LoRA weights (single sample)\n",
    "            voice_model.apply_lora_weights(lora_weights, batch_idx=0)\n",
    "            \n",
    "            # Tokenize input text\n",
    "            inputs = base_tokenizer(\n",
    "                [input_text],\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True\n",
    "            ).to(device)\n",
    "            style_inputs = base_tokenizer(DUMMY, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "            # Generate audio\n",
    "            generation = peft_model.generate(\n",
    "                input_ids=inputs.input_ids,\n",
    "                attention_mask=inputs.attention_mask,\n",
    "                prompt_input_ids=style_inputs.input_ids,\n",
    "                prompt_attention_mask=style_inputs.attention_mask,\n",
    "                do_sample=True,\n",
    "                temperature=1.0,\n",
    "                max_length=2000,\n",
    "            )\n",
    "            \n",
    "            # Decode to audio\n",
    "            audio_arr = generation.squeeze().cpu().numpy()\n",
    "            generated_audio = torch.from_numpy(audio_arr).squeeze().to(device)\n",
    "            \n",
    "            # Align lengths for MFCC comparison\n",
    "            min_len = min(generated_audio.shape[-1], target_audio.shape[-1])\n",
    "            gen_audio_aligned = generated_audio[..., :min_len]\n",
    "            tgt_audio_aligned = target_audio[..., :min_len]\n",
    "            \n",
    "            # Add batch dimension for loss function\n",
    "            gen_batch = gen_audio_aligned.unsqueeze(0)\n",
    "            tgt_batch = tgt_audio_aligned.unsqueeze(0)\n",
    "            \n",
    "            # Compute MFCC loss\n",
    "            loss, loss_dict = mfcc_loss_fn(gen_batch, tgt_batch)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            nn.utils.clip_grad_norm_(hypernetwork.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate losses\n",
    "            batch_loss += loss.item()\n",
    "            batch_mfcc_l1 += loss_dict['mfcc_l1']\n",
    "            batch_cosine += loss_dict['cosine_loss']\n",
    "        \n",
    "        # Average losses over batch\n",
    "        if len(batch) > 0:\n",
    "            batch_loss /= len(batch)\n",
    "            batch_mfcc_l1 /= len(batch)\n",
    "            batch_cosine /= len(batch)\n",
    "            \n",
    "            epoch_loss += batch_loss\n",
    "            epoch_mfcc_l1 += batch_mfcc_l1\n",
    "            epoch_cosine += batch_cosine\n",
    "            \n",
    "            scheduler.step()\n",
    "            global_step += 1\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{batch_loss:.4f}\",\n",
    "                'mfcc_l1': f\"{batch_mfcc_l1:.4f}\",\n",
    "                'cosine': f\"{batch_cosine:.4f}\",\n",
    "                'lr': f\"{scheduler.get_last_lr()[0]:.2e}\"\n",
    "            })\n",
    "            \n",
    "            # Periodic logging\n",
    "            if global_step % LOG_STEPS == 0:\n",
    "                avg_loss = epoch_loss / (batch_idx + 1)\n",
    "                avg_mfcc = epoch_mfcc_l1 / (batch_idx + 1)\n",
    "                avg_cosine = epoch_cosine / (batch_idx + 1)\n",
    "                print(f\"\\nStep {global_step}:\")\n",
    "                print(f\"  Avg Total Loss: {avg_loss:.4f}\")\n",
    "                print(f\"  Avg MFCC L1: {avg_mfcc:.4f}\")\n",
    "                print(f\"  Avg Cosine: {avg_cosine:.4f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if global_step % SAVE_STEPS == 0:\n",
    "                checkpoint_path = os.path.join(OUTPUT_DIR, f\"checkpoint-{global_step}\")\n",
    "                os.makedirs(checkpoint_path, exist_ok=True)\n",
    "                \n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'global_step': global_step,\n",
    "                    'hypernetwork_state_dict': hypernetwork.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'avg_loss': avg_loss,\n",
    "                }, os.path.join(checkpoint_path, 'model.pt'))\n",
    "                \n",
    "                print(f\"\\nCheckpoint saved at step {global_step}\")\n",
    "    \n",
    "    # End of epoch\n",
    "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "    avg_epoch_mfcc = epoch_mfcc_l1 / len(train_dataloader)\n",
    "    avg_epoch_cosine = epoch_cosine / len(train_dataloader)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1} completed:\")\n",
    "    print(f\"  Average Total Loss: {avg_epoch_loss:.4f}\")\n",
    "    print(f\"  Average MFCC L1: {avg_epoch_mfcc:.4f}\")\n",
    "    print(f\"  Average Cosine: {avg_epoch_cosine:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if avg_epoch_loss < best_loss:\n",
    "        best_loss = avg_epoch_loss\n",
    "        best_model_path = os.path.join(OUTPUT_DIR, \"best_model\")\n",
    "        os.makedirs(best_model_path, exist_ok=True)\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'hypernetwork_state_dict': hypernetwork.state_dict(),\n",
    "            'best_loss': best_loss,\n",
    "        }, os.path.join(best_model_path, 'model.pt'))\n",
    "        \n",
    "        print(f\"Best model saved with loss: {best_loss:.4f}\")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9df07b2",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0be980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for inference\n",
    "def load_best_model():\n",
    "    best_model_path = os.path.join(OUTPUT_DIR, \"best_model\", \"model.pt\")\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    hypernetwork.load_state_dict(checkpoint['hypernetwork_state_dict'])\n",
    "    print(f\"Loaded best model from epoch {checkpoint['epoch']} with loss {checkpoint['best_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d207903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_best_model()\n",
    "voice_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc188b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference\n",
    "test_voice_desc = [\"A clear, young female voice with a British accent\"]\n",
    "test_text = [\"Hello, this is a test of the dynamic voice adaptation system.\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = voice_model(\n",
    "        voice_descriptions=test_voice_desc,\n",
    "        input_texts=test_text,\n",
    "        target_audios=None\n",
    "    )\n",
    "    \n",
    "    generated_audio = outputs['generated_audios'][0]\n",
    "\n",
    "print(\"Inference completed successfully!\")\n",
    "print(f\"Generated audio shape: {generated_audio.shape}\")\n",
    "\n",
    "# Play generated audio\n",
    "Audio(generated_audio.cpu().numpy(), rate=24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b300c345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize MFCC comparison\n",
    "def visualize_mfcc_comparison(generated, target, sr=24000):\n",
    "    \"\"\"Visualize MFCC features of generated vs target audio\"\"\"\n",
    "    mfcc_transform = T.MFCC(\n",
    "        sample_rate=sr,\n",
    "        n_mfcc=40,\n",
    "        melkwargs={'n_fft': 1024, 'hop_length': 256, 'n_mels': 80}\n",
    "    )\n",
    "    \n",
    "    # Ensure same length\n",
    "    min_len = min(generated.shape[-1], target.shape[-1])\n",
    "    gen = generated[..., :min_len]\n",
    "    tgt = target[..., :min_len]\n",
    "    \n",
    "    # Compute MFCCs\n",
    "    gen_mfcc = mfcc_transform(gen.unsqueeze(0)).squeeze(0).cpu().numpy()\n",
    "    tgt_mfcc = mfcc_transform(tgt.unsqueeze(0)).squeeze(0).cpu().numpy()\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    \n",
    "    im1 = axes[0].imshow(gen_mfcc, aspect='auto', origin='lower', cmap='viridis')\n",
    "    axes[0].set_title('Generated Audio MFCC')\n",
    "    axes[0].set_ylabel('MFCC Coefficient')\n",
    "    plt.colorbar(im1, ax=axes[0])\n",
    "    \n",
    "    im2 = axes[1].imshow(tgt_mfcc, aspect='auto', origin='lower', cmap='viridis')\n",
    "    axes[1].set_title('Target Audio MFCC')\n",
    "    axes[1].set_ylabel('MFCC Coefficient')\n",
    "    axes[1].set_xlabel('Time Frame')\n",
    "    plt.colorbar(im2, ax=axes[1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Test with a sample\n",
    "sample = datasets[0]\n",
    "test_desc = [sample['style_prompt']]\n",
    "test_txt = [sample['content_prompt']]\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = voice_model(\n",
    "        voice_descriptions=test_desc,\n",
    "        input_texts=test_txt,\n",
    "        target_audios=None\n",
    "    )\n",
    "    gen_audio = out['generated_audios'][0]\n",
    "\n",
    "visualize_mfcc_comparison(gen_audio.cpu(), sample['audio'], sr=24000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t2a-lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
