{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "464a6599",
   "metadata": {},
   "source": [
    "# LatentForge TTS 다중 GPU 학습 노트북 (Accelerate 사용)\n",
    "\n",
    "이 노트북은 `accelerate` 라이브러리의 `notebook_launcher`를 사용하여 Jupyter 환경에서 다중 GPU 학습을 실행하는 방법을 보여줍니다.\n",
    "\n",
    "`.py` 파일로 모듈화된 코드를 가져와서 사용하므로, 코드의 재사용성과 관리 용이성을 높일 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35180c0",
   "metadata": {},
   "source": [
    "- CUDA VISIBLE DEVICE 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9679e201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# CUDA 디바이스 설정 (torch 임포트 전에 해야 함!)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # 사용할 GPU 번호들 #Ex) \"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb916f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Available GPUs: 1\n",
      "GPU 0: NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83194fc",
   "metadata": {},
   "source": [
    "## 1. 환경 설정 및 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b06800d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모든 모듈 임포트 성공!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 프로젝트의 src 폴더를 Python 경로에 추가\n",
    "project_root = Path.cwd()\n",
    "src_path = project_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "# 필요한 모듈 임포트 (상대 임포트 문제를 피하기 위해 직접 경로 사용)\n",
    "from config import TrainingConfig\n",
    "from data_loading.dataset import create_dataloader\n",
    "from models.tts_lora import TTSWithLoRA\n",
    "from training.losses import CombinedLoss  # 직접 losses 모듈에서 가져오기\n",
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "print(\"✅ 모든 모듈 임포트 성공!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "186049a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"Set seed for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7820ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전역 seed 설정\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61d0587",
   "metadata": {},
   "source": [
    "## 2. 학습 함수 정의\n",
    "\n",
    "`accelerate`를 사용하려면 모든 학습 로직을 하나의 함수 안에 캡슐화해야 합니다. `notebook_launcher`가 이 함수를 각 GPU 프로세스에서 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a885ffab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_function(config: TrainingConfig):\n",
    "    \"\"\"다중 GPU 학습을 위한 메인 함수\"\"\"\n",
    "    \n",
    "    # 1. Accelerator 초기화\n",
    "    # Accelerator가 자동으로 장치 할당, 모델 및 데이터 병렬 처리를 관리합니다.\n",
    "    accelerator = Accelerator()\n",
    "    print(f\"Process {accelerator.process_index} starting on device: {accelerator.device}\")\n",
    "\n",
    "    # 2. 데이터 로더 생성\n",
    "    # 메인 프로세스에서만 데이터를 다운로드하도록 설정 (accelerator.is_main_process)\n",
    "    if accelerator.is_main_process:\n",
    "        print(\"Creating dataloaders...\")\n",
    "    train_dataloader = create_dataloader(\n",
    "        dataset_name=config.dataset_name,\n",
    "        split=\"train.clean.100\", # 데모를 위해 작은 데이터셋 사용\n",
    "        batch_size=config.batch_size,\n",
    "        sample_rate=config.sample_rate,\n",
    "        cache_dir=config.cache_dir,\n",
    "        seed = 42,\n",
    "    )\n",
    "    # val_dataloader = ... (필요시 추가)\n",
    "\n",
    "    # 3. 모델, 옵티마이저, 손실 함수 초기화\n",
    "    if accelerator.is_main_process:\n",
    "        print(\"Initializing model, optimizer, and loss function...\")\n",
    "        \n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.FEATURE_EXTRACTION,\n",
    "        r=config.lora_r,\n",
    "        lora_alpha=config.lora_alpha,\n",
    "        lora_dropout=config.lora_dropout,\n",
    "        target_modules=config.target_modules\n",
    "    )\n",
    "\n",
    "    hypernetwork_config = {\n",
    "        'hidden_dim': config.hypernetwork_hidden_dim,\n",
    "        'num_layers': config.hypernetwork_num_layers,\n",
    "    }\n",
    "    model = TTSWithLoRA(\n",
    "        tts_model_name=config.tts_model_name,\n",
    "        clap_model_name=config.clap_model_name,\n",
    "        qwen_model_name=config.qwen_model_name,\n",
    "        lora_config=lora_config,\n",
    "        hypernetwork_config=hypernetwork_config,\n",
    "        manual_target_modules=config.target_modules,\n",
    "    )\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "    loss_fn = CombinedLoss(sample_rate=config.sample_rate)\n",
    "\n",
    "    # 4. accelerator.prepare()로 모든 객체 래핑\n",
    "    # 이 단계를 통해 모델, 옵티마이저, 데이터 로더가 분산 환경에 맞게 준비됩니다.\n",
    "    model, optimizer, train_dataloader = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader\n",
    "    )\n",
    "\n",
    "    # 5. 학습 루프\n",
    "    if accelerator.is_main_process:\n",
    "        print(\"Starting training...\")\n",
    "        \n",
    "    progress_bar = tqdm(range(config.num_epochs * len(train_dataloader)), disable=not accelerator.is_local_main_process)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        total_loss = 0\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # 데이터셋에서 필요한 정보 추출\n",
    "            target_audio = batch['audio']  # 원본 음성\n",
    "            ## 디버깅 코드 ---------------------------------------------\n",
    "            # print(\"type(batch): \", type(batch))\n",
    "            # print(\"batch: \", batch)\n",
    "            content_text = batch['text']  # TTS에 입력할 대사\n",
    "            \n",
    "            # Speaker/Style prompt를 결합하여 CLAP 인코더에 전달\n",
    "            speaker_descriptions = batch[\"speaker_descriptions\"]\n",
    "            \n",
    "            # HyperNetwork가 CLAP 임베딩을 기반으로 LoRA 가중치를 생성하고,\n",
    "            # 생성된 LoRA가 적용된 TTS 모델로 content_prompt를 음성으로 합성\n",
    "            generated_audio = model(\n",
    "                content_text=content_text,\n",
    "                speaker_audio=target_audio,  # reference audio for CLAP audio encoding\n",
    "                speaker_text=speaker_descriptions,  # combined speaker + style prompts\n",
    "                sample_rate=config.sample_rate\n",
    "            )\n",
    "            \n",
    "            # 합성된 음성과 원본 음성 비교하여 Loss 계산\n",
    "            loss_dict = loss_fn(\n",
    "                generated_audio=generated_audio,\n",
    "                target_audio=target_audio,\n",
    "                reference_audio=target_audio\n",
    "            )\n",
    "            loss = loss_dict['total']\n",
    "            \n",
    "            # 역전파 - HyperNetwork의 파라미터만 업데이트됨\n",
    "            accelerator.backward(loss)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # 로그 출력 (매 10 스텝마다)\n",
    "            if step % 10 == 0 and accelerator.is_main_process:\n",
    "                print(f\"Epoch {epoch+1}, Step {step}: Loss = {loss.item():.4f}\")\n",
    "        \n",
    "        # 에포크별 평균 손실 출력\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        accelerator.print(f\"Epoch {epoch+1}/{config.num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 학습 완료 후 모델 저장 (메인 프로세스에서만)\n",
    "    if accelerator.is_main_process:\n",
    "        print(\"Training finished. Saving model...\")\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        output_path = Path(config.output_dir) / \"final_model.pt\"\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(unwrapped_model.state_dict(), output_path)\n",
    "        print(f\"Model saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61395eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['decoder.layers.0.cross_attention.k_proj', 'decoder.layers.0.cross_attention.o_proj', 'decoder.layers.0.cross_attention.q_proj', 'decoder.layers.0.cross_attention.v_proj', 'decoder.layers.0.self_attention.k_proj', 'decoder.layers.0.self_attention.o_proj', 'decoder.layers.0.self_attention.q_proj', 'decoder.layers.0.self_attention.v_proj', 'decoder.layers.1.cross_attention.k_proj', 'decoder.layers.1.cross_attention.o_proj', 'decoder.layers.1.cross_attention.q_proj', 'decoder.layers.1.cross_attention.v_proj', 'decoder.layers.1.self_attention.k_proj', 'decoder.layers.1.self_attention.o_proj', 'decoder.layers.1.self_attention.q_proj', 'decoder.layers.1.self_attention.v_proj', 'decoder.layers.2.cross_attention.k_proj', 'decoder.layers.2.cross_attention.o_proj', 'decoder.layers.2.cross_attention.q_proj', 'decoder.layers.2.cross_attention.v_proj', 'decoder.layers.2.self_attention.k_proj', 'decoder.layers.2.self_attention.o_proj', 'decoder.layers.2.self_attention.q_proj', 'decoder.layers.2.self_attention.v_proj', 'decoder.layers.3.cross_attention.k_proj', 'decoder.layers.3.cross_attention.o_proj', 'decoder.layers.3.cross_attention.q_proj', 'decoder.layers.3.cross_attention.v_proj', 'decoder.layers.3.self_attention.k_proj', 'decoder.layers.3.self_attention.o_proj', 'decoder.layers.3.self_attention.q_proj', 'decoder.layers.3.self_attention.v_proj', 'decoder.layers.4.cross_attention.k_proj', 'decoder.layers.4.cross_attention.o_proj', 'decoder.layers.4.cross_attention.q_proj', 'decoder.layers.4.cross_attention.v_proj', 'decoder.layers.4.self_attention.k_proj', 'decoder.layers.4.self_attention.o_proj', 'decoder.layers.4.self_attention.q_proj', 'decoder.layers.4.self_attention.v_proj', 'decoder.layers.5.cross_attention.k_proj', 'decoder.layers.5.cross_attention.o_proj', 'decoder.layers.5.cross_attention.q_proj', 'decoder.layers.5.cross_attention.v_proj', 'decoder.layers.5.self_attention.k_proj', 'decoder.layers.5.self_attention.o_proj', 'decoder.layers.5.self_attention.q_proj', 'decoder.layers.5.self_attention.v_proj', 'decoder.layers.6.cross_attention.k_proj', 'decoder.layers.6.cross_attention.o_proj', 'decoder.layers.6.cross_attention.q_proj', 'decoder.layers.6.cross_attention.v_proj', 'decoder.layers.6.self_attention.k_proj', 'decoder.layers.6.self_attention.o_proj', 'decoder.layers.6.self_attention.q_proj', 'decoder.layers.6.self_attention.v_proj', 'decoder.layers.7.cross_attention.k_proj', 'decoder.layers.7.cross_attention.o_proj', 'decoder.layers.7.cross_attention.q_proj', 'decoder.layers.7.cross_attention.v_proj', 'decoder.layers.7.self_attention.k_proj', 'decoder.layers.7.self_attention.o_proj', 'decoder.layers.7.self_attention.q_proj', 'decoder.layers.7.self_attention.v_proj', 'decoder.layers.8.cross_attention.k_proj', 'decoder.layers.8.cross_attention.o_proj', 'decoder.layers.8.cross_attention.q_proj', 'decoder.layers.8.cross_attention.v_proj', 'decoder.layers.8.self_attention.k_proj', 'decoder.layers.8.self_attention.o_proj', 'decoder.layers.8.self_attention.q_proj', 'decoder.layers.8.self_attention.v_proj', 'decoder.layers.9.cross_attention.k_proj', 'decoder.layers.9.cross_attention.o_proj', 'decoder.layers.9.cross_attention.q_proj', 'decoder.layers.9.cross_attention.v_proj', 'decoder.layers.9.self_attention.k_proj', 'decoder.layers.9.self_attention.o_proj', 'decoder.layers.9.self_attention.q_proj', 'decoder.layers.9.self_attention.v_proj', 'decoder.layers.10.cross_attention.k_proj', 'decoder.layers.10.cross_attention.o_proj', 'decoder.layers.10.cross_attention.q_proj', 'decoder.layers.10.cross_attention.v_proj', 'decoder.layers.10.self_attention.k_proj', 'decoder.layers.10.self_attention.o_proj', 'decoder.layers.10.self_attention.q_proj', 'decoder.layers.10.self_attention.v_proj', 'decoder.layers.11.cross_attention.k_proj', 'decoder.layers.11.cross_attention.o_proj', 'decoder.layers.11.cross_attention.q_proj', 'decoder.layers.11.cross_attention.v_proj', 'decoder.layers.11.self_attention.k_proj', 'decoder.layers.11.self_attention.o_proj', 'decoder.layers.11.self_attention.q_proj', 'decoder.layers.11.self_attention.v_proj', 'decoder.layers.12.cross_attention.k_proj', 'decoder.layers.12.cross_attention.o_proj', 'decoder.layers.12.cross_attention.q_proj', 'decoder.layers.12.cross_attention.v_proj', 'decoder.layers.12.self_attention.k_proj', 'decoder.layers.12.self_attention.o_proj', 'decoder.layers.12.self_attention.q_proj', 'decoder.layers.12.self_attention.v_proj', 'decoder.layers.13.cross_attention.k_proj', 'decoder.layers.13.cross_attention.o_proj', 'decoder.layers.13.cross_attention.q_proj', 'decoder.layers.13.cross_attention.v_proj', 'decoder.layers.13.self_attention.k_proj', 'decoder.layers.13.self_attention.o_proj', 'decoder.layers.13.self_attention.q_proj', 'decoder.layers.13.self_attention.v_proj', 'decoder.layers.14.cross_attention.k_proj', 'decoder.layers.14.cross_attention.o_proj', 'decoder.layers.14.cross_attention.q_proj', 'decoder.layers.14.cross_attention.v_proj', 'decoder.layers.14.self_attention.k_proj', 'decoder.layers.14.self_attention.o_proj', 'decoder.layers.14.self_attention.q_proj', 'decoder.layers.14.self_attention.v_proj', 'decoder.layers.15.cross_attention.k_proj', 'decoder.layers.15.cross_attention.o_proj', 'decoder.layers.15.cross_attention.q_proj', 'decoder.layers.15.cross_attention.v_proj', 'decoder.layers.15.self_attention.k_proj', 'decoder.layers.15.self_attention.o_proj', 'decoder.layers.15.self_attention.q_proj', 'decoder.layers.15.self_attention.v_proj', 'decoder.layers.16.cross_attention.k_proj', 'decoder.layers.16.cross_attention.o_proj', 'decoder.layers.16.cross_attention.q_proj', 'decoder.layers.16.cross_attention.v_proj', 'decoder.layers.16.self_attention.k_proj', 'decoder.layers.16.self_attention.o_proj', 'decoder.layers.16.self_attention.q_proj', 'decoder.layers.16.self_attention.v_proj']\n"
     ]
    }
   ],
   "source": [
    "# LoRA 타겟 설정\n",
    "manual_targets = []\n",
    "\n",
    "# 민관: 이유는 모르것는데, 18로 하면 오류 생김. 그래서 17로 해둠.\n",
    "# 모델 구조 봤을 때는, 레이어가 18개 이기는 함.\n",
    "for i in range(17):\n",
    "   manual_targets.append(f\"decoder.layers.{i}.cross_attention.k_proj\")\n",
    "   manual_targets.append(f\"decoder.layers.{i}.cross_attention.o_proj\")\n",
    "   manual_targets.append(f\"decoder.layers.{i}.cross_attention.q_proj\")\n",
    "   manual_targets.append(f\"decoder.layers.{i}.cross_attention.v_proj\")\n",
    "   manual_targets.append(f\"decoder.layers.{i}.self_attention.k_proj\")\n",
    "   manual_targets.append(f\"decoder.layers.{i}.self_attention.o_proj\")\n",
    "   manual_targets.append(f\"decoder.layers.{i}.self_attention.q_proj\")\n",
    "   manual_targets.append(f\"decoder.layers.{i}.self_attention.v_proj\")\n",
    "\n",
    "print(manual_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a34689",
   "metadata": {},
   "source": [
    "## 3. 학습 실행\n",
    "\n",
    "`notebook_launcher`를 사용하여 위에서 정의한 `training_function`을 실행합니다. `num_processes` 인자를 통해 사용할 GPU 수를 지정할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dff654",
   "metadata": {},
   "source": [
    "- 원래 사용하던 학습 스크립트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54713355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 GPUs.\n",
      "Launching training on one GPU.\n",
      "Process 0 starting on device: cuda\n",
      "Creating dataloaders...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153b1f5d465d4398b24038baf7910612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266ead887e8d465296e1e69e08c9d184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e52635db8f462299997ef8c8bc44eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf3a85eba0a4831a5e5fd7f9c92f3aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef8dda836788491f97f10af3c064ee86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 32182 samples from tictap11/libritts_p_dataset_20250821_095157\n",
      "Initializing model, optimizer, and loss function...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "231966d35aec474cbfe48457003470bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG][Hypernetwork] validated targets: 136 kept, 0 missing\n",
      "   e.g. ['decoder.layers.0.cross_attention.k_proj', 'decoder.layers.0.cross_attention.o_proj', 'decoder.layers.0.cross_attention.q_proj', 'decoder.layers.0.cross_attention.v_proj']\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b2a6771dbd423e975b4717e7a97f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] --- Processing batch item 0 ---\n",
      "[DEBUG] Speaker embedding shape: torch.Size([1, 1024])\n",
      "[DEBUG][Hypernetwork] target modules count -> self_attn: 68, cross_attn: 68\n",
      "[DEBUG] Generated LoRA weights shape (A, B for first module): torch.Size([1, 16, 1024]), torch.Size([1, 2048, 16])\n",
      "[DEBUG] LoRA weights shape after un-batching: torch.Size([16, 1024]), torch.Size([2048, 16])\n",
      "[DEBUG] --- Applying LoRA Weights ---\n",
      "[DEBUG] LoRA weight keys: ['decoder.layers.0.cross_attention.k_proj', 'decoder.layers.0.cross_attention.o_proj', 'decoder.layers.0.cross_attention.q_proj', 'decoder.layers.0.cross_attention.v_proj', 'decoder.layers.0.self_attention.k_proj', 'decoder.layers.0.self_attention.o_proj', 'decoder.layers.0.self_attention.q_proj', 'decoder.layers.0.self_attention.v_proj', 'decoder.layers.1.cross_attention.k_proj', 'decoder.layers.1.cross_attention.o_proj', 'decoder.layers.1.cross_attention.q_proj', 'decoder.layers.1.cross_attention.v_proj', 'decoder.layers.1.self_attention.k_proj', 'decoder.layers.1.self_attention.o_proj', 'decoder.layers.1.self_attention.q_proj', 'decoder.layers.1.self_attention.v_proj', 'decoder.layers.2.cross_attention.k_proj', 'decoder.layers.2.cross_attention.o_proj', 'decoder.layers.2.cross_attention.q_proj', 'decoder.layers.2.cross_attention.v_proj', 'decoder.layers.2.self_attention.k_proj', 'decoder.layers.2.self_attention.o_proj', 'decoder.layers.2.self_attention.q_proj', 'decoder.layers.2.self_attention.v_proj', 'decoder.layers.3.cross_attention.k_proj', 'decoder.layers.3.cross_attention.o_proj', 'decoder.layers.3.cross_attention.q_proj', 'decoder.layers.3.cross_attention.v_proj', 'decoder.layers.3.self_attention.k_proj', 'decoder.layers.3.self_attention.o_proj', 'decoder.layers.3.self_attention.q_proj', 'decoder.layers.3.self_attention.v_proj', 'decoder.layers.4.cross_attention.k_proj', 'decoder.layers.4.cross_attention.o_proj', 'decoder.layers.4.cross_attention.q_proj', 'decoder.layers.4.cross_attention.v_proj', 'decoder.layers.4.self_attention.k_proj', 'decoder.layers.4.self_attention.o_proj', 'decoder.layers.4.self_attention.q_proj', 'decoder.layers.4.self_attention.v_proj', 'decoder.layers.5.cross_attention.k_proj', 'decoder.layers.5.cross_attention.o_proj', 'decoder.layers.5.cross_attention.q_proj', 'decoder.layers.5.cross_attention.v_proj', 'decoder.layers.5.self_attention.k_proj', 'decoder.layers.5.self_attention.o_proj', 'decoder.layers.5.self_attention.q_proj', 'decoder.layers.5.self_attention.v_proj', 'decoder.layers.6.cross_attention.k_proj', 'decoder.layers.6.cross_attention.o_proj', 'decoder.layers.6.cross_attention.q_proj', 'decoder.layers.6.cross_attention.v_proj', 'decoder.layers.6.self_attention.k_proj', 'decoder.layers.6.self_attention.o_proj', 'decoder.layers.6.self_attention.q_proj', 'decoder.layers.6.self_attention.v_proj', 'decoder.layers.7.cross_attention.k_proj', 'decoder.layers.7.cross_attention.o_proj', 'decoder.layers.7.cross_attention.q_proj', 'decoder.layers.7.cross_attention.v_proj', 'decoder.layers.7.self_attention.k_proj', 'decoder.layers.7.self_attention.o_proj', 'decoder.layers.7.self_attention.q_proj', 'decoder.layers.7.self_attention.v_proj', 'decoder.layers.8.cross_attention.k_proj', 'decoder.layers.8.cross_attention.o_proj', 'decoder.layers.8.cross_attention.q_proj', 'decoder.layers.8.cross_attention.v_proj', 'decoder.layers.8.self_attention.k_proj', 'decoder.layers.8.self_attention.o_proj', 'decoder.layers.8.self_attention.q_proj', 'decoder.layers.8.self_attention.v_proj', 'decoder.layers.9.cross_attention.k_proj', 'decoder.layers.9.cross_attention.o_proj', 'decoder.layers.9.cross_attention.q_proj', 'decoder.layers.9.cross_attention.v_proj', 'decoder.layers.9.self_attention.k_proj', 'decoder.layers.9.self_attention.o_proj', 'decoder.layers.9.self_attention.q_proj', 'decoder.layers.9.self_attention.v_proj', 'decoder.layers.10.cross_attention.k_proj', 'decoder.layers.10.cross_attention.o_proj', 'decoder.layers.10.cross_attention.q_proj', 'decoder.layers.10.cross_attention.v_proj', 'decoder.layers.10.self_attention.k_proj', 'decoder.layers.10.self_attention.o_proj', 'decoder.layers.10.self_attention.q_proj', 'decoder.layers.10.self_attention.v_proj', 'decoder.layers.11.cross_attention.k_proj', 'decoder.layers.11.cross_attention.o_proj', 'decoder.layers.11.cross_attention.q_proj', 'decoder.layers.11.cross_attention.v_proj', 'decoder.layers.11.self_attention.k_proj', 'decoder.layers.11.self_attention.o_proj', 'decoder.layers.11.self_attention.q_proj', 'decoder.layers.11.self_attention.v_proj', 'decoder.layers.12.cross_attention.k_proj', 'decoder.layers.12.cross_attention.o_proj', 'decoder.layers.12.cross_attention.q_proj', 'decoder.layers.12.cross_attention.v_proj', 'decoder.layers.12.self_attention.k_proj', 'decoder.layers.12.self_attention.o_proj', 'decoder.layers.12.self_attention.q_proj', 'decoder.layers.12.self_attention.v_proj', 'decoder.layers.13.cross_attention.k_proj', 'decoder.layers.13.cross_attention.o_proj', 'decoder.layers.13.cross_attention.q_proj', 'decoder.layers.13.cross_attention.v_proj', 'decoder.layers.13.self_attention.k_proj', 'decoder.layers.13.self_attention.o_proj', 'decoder.layers.13.self_attention.q_proj', 'decoder.layers.13.self_attention.v_proj', 'decoder.layers.14.cross_attention.k_proj', 'decoder.layers.14.cross_attention.o_proj', 'decoder.layers.14.cross_attention.q_proj', 'decoder.layers.14.cross_attention.v_proj', 'decoder.layers.14.self_attention.k_proj', 'decoder.layers.14.self_attention.o_proj', 'decoder.layers.14.self_attention.q_proj', 'decoder.layers.14.self_attention.v_proj', 'decoder.layers.15.cross_attention.k_proj', 'decoder.layers.15.cross_attention.o_proj', 'decoder.layers.15.cross_attention.q_proj', 'decoder.layers.15.cross_attention.v_proj', 'decoder.layers.15.self_attention.k_proj', 'decoder.layers.15.self_attention.o_proj', 'decoder.layers.15.self_attention.q_proj', 'decoder.layers.15.self_attention.v_proj', 'decoder.layers.16.cross_attention.k_proj', 'decoder.layers.16.cross_attention.o_proj', 'decoder.layers.16.cross_attention.q_proj', 'decoder.layers.16.cross_attention.v_proj', 'decoder.layers.16.self_attention.k_proj', 'decoder.layers.16.self_attention.o_proj', 'decoder.layers.16.self_attention.q_proj', 'decoder.layers.16.self_attention.v_proj']\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.0.self_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.0.self_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.0.self_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.0.self_attention.o_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.0.cross_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.0.cross_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.0.cross_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.0.cross_attention.o_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.1.self_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.1.self_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.1.self_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.1.self_attention.o_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.1.cross_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.1.cross_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.1.cross_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.1.cross_attention.o_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.2.self_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.2.self_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.2.self_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.2.self_attention.o_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.2.cross_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.2.cross_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.2.cross_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.2.cross_attention.o_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.3.self_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.3.self_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.3.self_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.3.self_attention.o_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.3.cross_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.3.cross_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.3.cross_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.3.cross_attention.o_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.4.self_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.4.self_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.4.self_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.4.self_attention.o_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.4.cross_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.4.cross_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.4.cross_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.4.cross_attention.o_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.5.self_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.5.self_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.5.self_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.5.self_attention.o_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.5.cross_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.5.cross_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.5.cross_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.5.cross_attention.o_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.6.self_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.6.self_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.6.self_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.6.self_attention.o_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.6.cross_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.6.cross_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.6.cross_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.6.cross_attention.o_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.7.self_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.7.self_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.7.self_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.7.self_attention.o_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.7.cross_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.7.cross_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.7.cross_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.7.cross_attention.o_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.8.self_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.8.self_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.8.self_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.8.self_attention.o_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.8.cross_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.8.cross_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.8.cross_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.8.cross_attention.o_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.9.self_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.9.self_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.9.self_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.9.self_attention.o_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.9.cross_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.9.cross_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.9.cross_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.9.cross_attention.o_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.10.self_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.10.self_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.10.self_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.10.self_attention.o_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.10.cross_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.10.cross_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.10.cross_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.10.cross_attention.o_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.11.self_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.11.self_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.11.self_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.11.self_attention.o_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.11.cross_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.11.cross_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.11.cross_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.11.cross_attention.o_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.12.self_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.12.self_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.12.self_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.12.self_attention.o_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.12.cross_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.12.cross_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.12.cross_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.12.cross_attention.o_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.13.self_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.13.self_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.13.self_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.13.self_attention.o_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.13.cross_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.13.cross_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.13.cross_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.13.cross_attention.o_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.14.self_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.14.self_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.14.self_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.14.self_attention.o_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.14.cross_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.14.cross_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.14.cross_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.14.cross_attention.o_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.15.self_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.15.self_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.15.self_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.15.self_attention.o_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.15.cross_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.15.cross_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.15.cross_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.15.cross_attention.o_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.16.self_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.16.self_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.16.self_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.16.self_attention.o_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.16.cross_attention.q_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.16.cross_attention.k_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.16.cross_attention.v_proj\n",
      "  -> SUCCESS: Applied weights to base_model.model.decoder.layers.16.cross_attention.o_proj\n",
      "[DEBUG] Total LoRA layers updated: 136\n",
      "[DEBUG] --- Finished Applying LoRA Weights ---\n",
      "[DEBUG] Raw model output shape: torch.Size([1, 16, 2048])\n",
      "Epoch 1, Step 0: Loss = 99.1091\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_gpus\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m GPUs.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# notebook_launcher로 학습 시작\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# 이 셀을 실행하면 지정된 수의 프로세스가 백그라운드에서 실행됩니다.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mnotebook_launcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_gpus\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mywslfolder/Workspace/Competitions_Linux/AI_Champion/LatentForge_Train/.venv/lib/python3.12/site-packages/accelerate/launchers.py:270\u001b[39m, in \u001b[36mnotebook_launcher\u001b[39m\u001b[34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes, rdzv_backend, rdzv_endpoint, rdzv_conf, rdzv_id, max_restarts, monitor_interval, log_line_prefix_template)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    269\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLaunching training on CPU.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 79\u001b[39m, in \u001b[36mtraining_function\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m     75\u001b[39m speaker_descriptions = batch[\u001b[33m\"\u001b[39m\u001b[33mspeaker_descriptions\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# HyperNetwork가 CLAP 임베딩을 기반으로 LoRA 가중치를 생성하고,\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# 생성된 LoRA가 적용된 TTS 모델로 content_prompt를 음성으로 합성\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m generated_audio = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontent_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontent_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspeaker_audio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_audio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# reference audio for CLAP audio encoding\u001b[39;49;00m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspeaker_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeaker_descriptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# combined speaker + style prompts\u001b[39;49;00m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample_rate\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# 합성된 음성과 원본 음성 비교하여 Loss 계산\u001b[39;00m\n\u001b[32m     87\u001b[39m loss_dict = loss_fn(\n\u001b[32m     88\u001b[39m     generated_audio=generated_audio,\n\u001b[32m     89\u001b[39m     target_audio=target_audio,\n\u001b[32m     90\u001b[39m     reference_audio=target_audio\n\u001b[32m     91\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mywslfolder/Workspace/Competitions_Linux/AI_Champion/LatentForge_Train/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mywslfolder/Workspace/Competitions_Linux/AI_Champion/LatentForge_Train/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mywslfolder/Workspace/Competitions_Linux/AI_Champion/LatentForge_Train/src/models/tts_lora.py:248\u001b[39m, in \u001b[36mTTSWithLoRA.forward\u001b[39m\u001b[34m(self, content_text, speaker_audio, speaker_text, sample_rate, style_prompts, content_prompts, labels, attention_mask, **tts_kwargs)\u001b[39m\n\u001b[32m    245\u001b[39m     tts_kwargs[\u001b[33m'\u001b[39m\u001b[33mspeech_inputs\u001b[39m\u001b[33m'\u001b[39m] = labels[i:i+\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m labels.shape[\u001b[32m0\u001b[39m] > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m labels\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.amp.autocast(device_type=\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtts_model_with_lora\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msingle_text_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtts_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(outputs, \u001b[33m'\u001b[39m\u001b[33mspectrogram\u001b[39m\u001b[33m'\u001b[39m): output_audio = outputs.spectrogram\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(outputs, \u001b[33m'\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m'\u001b[39m): output_audio = outputs.audio\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mywslfolder/Workspace/Competitions_Linux/AI_Champion/LatentForge_Train/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mywslfolder/Workspace/Competitions_Linux/AI_Champion/LatentForge_Train/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mywslfolder/Workspace/Competitions_Linux/AI_Champion/LatentForge_Train/.venv/lib/python3.12/site-packages/peft/peft_model.py:2898\u001b[39m, in \u001b[36mPeftModelForFeatureExtraction.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[39m\n\u001b[32m   2896\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(**kwargs):\n\u001b[32m   2897\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m2898\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2899\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2900\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2901\u001b[39m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2902\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2903\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2904\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2905\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2906\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2908\u001b[39m batch_size = _get_batch_size(input_ids, inputs_embeds)\n\u001b[32m   2909\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2910\u001b[39m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mywslfolder/Workspace/Competitions_Linux/AI_Champion/LatentForge_Train/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mywslfolder/Workspace/Competitions_Linux/AI_Champion/LatentForge_Train/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mywslfolder/Workspace/Competitions_Linux/AI_Champion/LatentForge_Train/.venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:222\u001b[39m, in \u001b[36mBaseTuner.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any):\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mywslfolder/Workspace/Competitions_Linux/AI_Champion/LatentForge_Train/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:959\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    957\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    958\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m959\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    961\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mywslfolder/Workspace/Competitions_Linux/AI_Champion/LatentForge_Train/.venv/lib/python3.12/site-packages/transformers/models/dia/modeling_dia.py:790\u001b[39m, in \u001b[36mDiaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, decoder_input_ids, decoder_position_ids, decoder_attention_mask, encoder_outputs, past_key_values, use_cache, output_attentions, output_hidden_states, cache_position, **kwargs)\u001b[39m\n\u001b[32m    787\u001b[39m     past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n\u001b[32m    789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[38;5;66;03m# If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput\u001b[39;00m\n\u001b[32m    798\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mywslfolder/Workspace/Competitions_Linux/AI_Champion/LatentForge_Train/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mywslfolder/Workspace/Competitions_Linux/AI_Champion/LatentForge_Train/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mywslfolder/Workspace/Competitions_Linux/AI_Champion/LatentForge_Train/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:959\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    957\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    958\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m959\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    961\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mywslfolder/Workspace/Competitions_Linux/AI_Champion/LatentForge_Train/.venv/lib/python3.12/site-packages/transformers/models/dia/modeling_dia.py:458\u001b[39m, in \u001b[36mDiaEncoder.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, output_attentions, output_hidden_states, **kwargs)\u001b[39m\n\u001b[32m    455\u001b[39m encoder_states = () \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    456\u001b[39m all_attentions = () \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m encoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayers\u001b[49m:\n\u001b[32m    459\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    460\u001b[39m         encoder_states = encoder_states + (hidden_states,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mywslfolder/Workspace/Competitions_Linux/AI_Champion/LatentForge_Train/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1949\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1944\u001b[39m         \u001b[38;5;28mself\u001b[39m._backward_pre_hooks = OrderedDict()\n\u001b[32m   1946\u001b[39m \u001b[38;5;66;03m# It is crucial that the return type is not annotated as `Any`, otherwise type checking\u001b[39;00m\n\u001b[32m   1947\u001b[39m \u001b[38;5;66;03m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[39;00m\n\u001b[32m   1948\u001b[39m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/115074\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> Union[Tensor, \u001b[33m\"\u001b[39m\u001b[33mModule\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1950\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m:\n\u001b[32m   1951\u001b[39m         _parameters = \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 학습 설정 로드\n",
    "config = TrainingConfig(\n",
    "    batch_size=1, # GPU 메모리에 맞게 조정\n",
    "    num_epochs=1, # 데모용 에포크 수\n",
    "    learning_rate=1e-5,\n",
    "    target_modules=manual_targets\n",
    ")\n",
    "\n",
    "# 사용할 GPU 수\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Found {num_gpus} GPUs.\")\n",
    "\n",
    "# notebook_launcher로 학습 시작\n",
    "# 이 셀을 실행하면 지정된 수의 프로세스가 백그라운드에서 실행됩니다.\n",
    "notebook_launcher(training_function, args=(config,), num_processes=num_gpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
